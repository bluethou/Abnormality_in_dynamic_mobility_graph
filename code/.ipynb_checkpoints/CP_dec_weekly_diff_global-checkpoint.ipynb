{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bluethou/Documents/TUM2020S/Thesis2020/2020-pooreumoe-msc-thesis/code'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "import tensortools as tt\n",
    "from tensortools.operations import unfold as tt_unfold, khatri_rao\n",
    "import tensorly as tl\n",
    "from tensorly import unfold as tl_unfold\n",
    "from tensorly.decomposition import parafac\n",
    "\n",
    "# import some useful functions (they are available in utils.py)\n",
    "# from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tensor\n",
    "with open('travel_tensor.pkl', 'rb') as f:\n",
    "    travel_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  2. -3. ...  0.  0.  0.]\n",
      " [ 2.  0.  3. ...  0. -1.  0.]\n",
      " [ 3.  3.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0. -1.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn diff\n",
    "\n",
    "diff_week_gl= np.array(difference(travel_data))\n",
    "print(diff_week_gl[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_data = diff_week_gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serial tensors by slicing the time interval.\n",
    "W = 7 # window size\n",
    "serial_tensors = []\n",
    "for i in range(len(travel_data)-W):\n",
    "    if i%7 ==0: # to obtain mutually exclusive tensors\n",
    "        temp = np.swapaxes(travel_data[i:i+5],0,1)\n",
    "        travel_NFT = np.swapaxes(temp, 1,2)\n",
    "        serial_tensors.append(travel_NFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # without axis change\n",
    "# W = 5 # window size\n",
    "# serial_tensors = []\n",
    "# for i in range(len(travel_data)-W):\n",
    "#     serial_tensors.append(travel_data[i:i+5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  2.  0. ...  0.  0.  0.]\n",
      " [ 2.  0. -2. ...  0.  0.  0.]\n",
      " [ 1. -1.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75, 150, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp = np.swapaxes(serial_tensors[0], 0,1)\n",
    "# travel_NFT = np.swapaxes(temp, 1,2)\n",
    "# travel_NFT.shape\n",
    "print(serial_tensors[0][:,:,1])\n",
    "print(\"----\")\n",
    "# print(tl.unfold(serial_tensors[0], mode=0).T)\n",
    "serial_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation: 0 | Loss (A): 0.044752390612576805 | Loss (C): 0.044717369989064226\n",
      "interation: 2 | Loss (A): 0.05463086296469487 | Loss (C): 0.054746338997740035\n",
      "interation: 4 | Loss (A): 0.08793511495075675 | Loss (C): 0.08800835793285519\n",
      "interation: 6 | Loss (A): 0.07340311310775431 | Loss (C): 0.0735162797773751\n",
      "interation: 8 | Loss (A): 0.07279180594266346 | Loss (C): 0.07277232892685523\n",
      "interation: 10 | Loss (A): 0.10632376947113872 | Loss (C): 0.10642360738978525\n",
      "interation: 12 | Loss (A): 0.12898431870147045 | Loss (C): 0.12875200946165907\n",
      "interation: 14 | Loss (A): 0.12968223873559284 | Loss (C): 0.12985613819447003\n",
      "interation: 16 | Loss (A): 0.17667865787392487 | Loss (C): 0.17679518846299971\n",
      "interation: 18 | Loss (A): 0.270858254027556 | Loss (C): 0.27065691873200526\n",
      "interation: 20 | Loss (A): 0.3164754227905997 | Loss (C): 0.3159315172312123\n",
      "interation: 22 | Loss (A): 0.07587196351101187 | Loss (C): 0.07590281553260117\n",
      "Average Loss of A:  0.13351397782082866 Average Loss of C 0.13346755763221188\n"
     ]
    }
   ],
   "source": [
    "rank =5\n",
    "\n",
    "A = np.random.random((serial_tensors[0].shape[0], rank)) # N node\n",
    "B = np.random.random((serial_tensors[0].shape[1], rank)) # F feature\n",
    "C = np.random.random((serial_tensors[0].shape[2], rank)) # T time\n",
    "\n",
    "A_array = []\n",
    "C_array = []\n",
    "Ct_B =0\n",
    "B_At =0\n",
    "alpha = 0.05\n",
    "beta = 0.05\n",
    "T_interval = len(serial_tensors)\n",
    "\n",
    "res_a_sum = 0\n",
    "res_c_sum = 0\n",
    "\n",
    "for t in range(T_interval):\n",
    "  # optimiza A\n",
    "    Ct_B = khatri_rao([C, B])\n",
    "    Xt_mode1_transpose = tl.unfold(serial_tensors[t], mode=0).T\n",
    "    X = np.concatenate((Xt_mode1_transpose, np.sqrt(alpha)*A.T), axis=0)\n",
    "    I = np.identity(A.shape[1])\n",
    "    F = np.concatenate((Ct_B, np.sqrt(alpha)*I))\n",
    "    A = np.linalg.lstsq(F,X, rcond=None)[0].T\n",
    "    A_array.append(A)\n",
    "\n",
    "    # optimize C\n",
    "    B_At = khatri_rao([B, A])\n",
    "    Xt_mode3_transpose = tl.unfold(serial_tensors[t], mode=2).T\n",
    "    X = np.concatenate((Xt_mode3_transpose, np.sqrt(beta)*C.T), axis=0)\n",
    "    I = np.identity(C.shape[1])\n",
    "    F = np.concatenate((B_At, np.sqrt(beta)*I))\n",
    "    C = np.linalg.lstsq(F,X, rcond=None)[0].T\n",
    "    C_array.append(C)\n",
    "\n",
    "    res_a = np.square(Ct_B.dot(A.T) - Xt_mode1_transpose)\n",
    "    res_c = np.square(B_At.dot(C.T) - Xt_mode3_transpose)\n",
    "    res_a_sum += res_a.mean()\n",
    "    res_c_sum += res_c.mean()\n",
    "    \n",
    "    if t % int(T_interval * .1) == 0:\n",
    "        print(\"interation:\", t, \"| Loss (A):\", res_a.mean(), \"| Loss (C):\", res_c.mean())\n",
    "        \n",
    "print(\"Average Loss of A: \",res_a_sum/len(A_array), \"Average Loss of C\",res_c_sum/len(C_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize B\n",
    "# X = np.concatenate(, axis=0)\n",
    "X_concat = tl.unfold(serial_tensors[0], mode=1).T\n",
    "for x in range(len(serial_tensors)):\n",
    "    if x>0:\n",
    "        XtT_unfold = tl.unfold(serial_tensors[x], mode=1).T\n",
    "        X_concat = np.concatenate((X_concat, XtT_unfold))\n",
    "\n",
    "F_concat = khatri_rao([C_array[0], A_array[0]])\n",
    "for i in range(len(C_array)):\n",
    "    if i >0:\n",
    "        F_concat = np.concatenate((F_concat, khatri_rao([C_array[i], A_array[i]])))\n",
    "        \n",
    "B = np.linalg.lstsq(F_concat,X_concat, rcond=None)[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_a = np.square(khatri_rao([C, B]).dot(A.T) - Xt_mode1_transpose)\n",
    "# res_c = np.square(khatri_rao([B, A]).dot(C.T) - Xt_mode3_transpose)\n",
    "# print(res_a.mean(), res_c.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss of A:  0.07628651160675731 Average Loss of C 0.0762569265740517\n"
     ]
    }
   ],
   "source": [
    "# Adjusted error measure\n",
    "res_a_sum = 0\n",
    "res_c_sum = 0\n",
    "\n",
    "for i in range(len(A_array)):\n",
    "    Ai = A_array[i]\n",
    "    Ci = C_array[i]\n",
    "    res_a_sum += np.square(khatri_rao([Ci, B]).dot(Ai.T) - Xt_mode1_transpose).mean()\n",
    "    res_c_sum += np.square(khatri_rao([B, Ai]).dot(Ci.T) - Xt_mode3_transpose).mean()\n",
    "\n",
    "print(\"Average Loss of A: \",res_a_sum/len(A_array), \"Average Loss of C\",res_c_sum/len(C_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "B_old =copy.deepcopy(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation: 0 | Loss (A): 0.04474145496696281 | Loss (C): 0.04472920629910687\n",
      "interation: 2 | Loss (A): 0.054289110542249004 | Loss (C): 0.05451344866330377\n",
      "interation: 4 | Loss (A): 0.08716343442000639 | Loss (C): 0.08747612865697964\n",
      "interation: 6 | Loss (A): 0.0730607715498489 | Loss (C): 0.0733495297039752\n",
      "interation: 8 | Loss (A): 0.07208867029464404 | Loss (C): 0.07249146701784721\n",
      "interation: 10 | Loss (A): 0.1057017660501977 | Loss (C): 0.10627635360018835\n",
      "interation: 12 | Loss (A): 0.12842642373947202 | Loss (C): 0.12870212206700382\n",
      "interation: 14 | Loss (A): 0.12876870635765186 | Loss (C): 0.12910303491787511\n",
      "interation: 16 | Loss (A): 0.1760169155523703 | Loss (C): 0.175713557995131\n",
      "interation: 18 | Loss (A): 0.26877366584833323 | Loss (C): 0.2708705074537939\n",
      "interation: 20 | Loss (A): 0.3154686662535808 | Loss (C): 0.31656368821600495\n",
      "interation: 22 | Loss (A): 0.07562425178784643 | Loss (C): 0.07587737466750563\n",
      "Average Loss of A:  0.13275759932387277 Average Loss of C 0.13319040523068892\n"
     ]
    }
   ],
   "source": [
    "# Re-optimize A & C \n",
    "rank =5\n",
    "\n",
    "A = A_array[0] # N node\n",
    "# B = np.random.random((serial_tensors[0].shape[1], rank)) # F feature\n",
    "C = C_array[0] # T time\n",
    "\n",
    "Aopt_array = []\n",
    "Copt_array = []\n",
    "Ct_B =0\n",
    "B_At =0\n",
    "alpha = 0.05\n",
    "beta = 0.05\n",
    "T_interval = len(serial_tensors)\n",
    "\n",
    "res_a_sum = 0\n",
    "res_c_sum = 0\n",
    "\n",
    "for t in range(T_interval):\n",
    "  # optimiza A\n",
    "    Ct_B = khatri_rao([C, B])\n",
    "    Xt_mode1_transpose = tl.unfold(serial_tensors[t], mode=0).T\n",
    "    X = np.concatenate((Xt_mode1_transpose, np.sqrt(alpha)*A.T), axis=0)\n",
    "    I = np.identity(A.shape[1])\n",
    "    F = np.concatenate((Ct_B, np.sqrt(alpha)*I))\n",
    "    A = np.linalg.lstsq(F,X, rcond=None)[0].T\n",
    "    Aopt_array.append(A)\n",
    "\n",
    "    # optimize C\n",
    "    B_At = khatri_rao([B, A])\n",
    "    Xt_mode3_transpose = tl.unfold(serial_tensors[t], mode=2).T\n",
    "    X = np.concatenate((Xt_mode3_transpose, np.sqrt(beta)*C.T), axis=0)\n",
    "    I = np.identity(C.shape[1])\n",
    "    F = np.concatenate((B_At, np.sqrt(beta)*I))\n",
    "    C = np.linalg.lstsq(F,X, rcond=None)[0].T\n",
    "    Copt_array.append(C)\n",
    "\n",
    "    res_a = np.square(Ct_B.dot(A.T) - Xt_mode1_transpose)\n",
    "    res_c = np.square(B_At.dot(C.T) - Xt_mode3_transpose)\n",
    "    res_a_sum += res_a.mean()\n",
    "    res_c_sum += res_c.mean()\n",
    "    \n",
    "    if t % int(T_interval * .1) == 0:\n",
    "        print(\"interation:\", t, \"| Loss (A):\", res_a.mean(), \"| Loss (C):\", res_c.mean())\n",
    "        \n",
    "print(\"Average Loss of A: \",res_a_sum/len(A_array), \"Average Loss of C\",res_c_sum/len(C_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-optimize B\n",
    "# # X = np.concatenate(, axis=0)\n",
    "# X_concat = tl.unfold(serial_tensors[0], mode=1).T\n",
    "# for x in range(len(serial_tensors)):\n",
    "#     if x>0:\n",
    "#         XtT_unfold = tl.unfold(serial_tensors[x], mode=1).T\n",
    "#         X_concat = np.concatenate((X_concat, XtT_unfold))\n",
    "\n",
    "# F_concat = khatri_rao([Copt_array[0], Aopt_array[0]])\n",
    "# for i in range(len(C_array)):\n",
    "#     if i >0:\n",
    "#         F_concat = np.concatenate((F_concat, khatri_rao([Copt_array[i], Aopt_array[i]])))\n",
    "        \n",
    "# B = np.linalg.lstsq(F_concat,X_concat, rcond=None)[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss of A:  0.07686687531632307 Average Loss of C 0.0769802962745731\n"
     ]
    }
   ],
   "source": [
    "# Adjusted error measure\n",
    "res_a_sum = 0\n",
    "res_c_sum = 0\n",
    "\n",
    "for i in range(len(A_array)):\n",
    "    Ai = Aopt_array[i]\n",
    "    Ci = Copt_array[i]\n",
    "    res_a_sum += np.square(khatri_rao([Ci, B]).dot(Ai.T) - Xt_mode1_transpose).mean()\n",
    "    res_c_sum += np.square(khatri_rao([B, Ai]).dot(Ci.T) - Xt_mode3_transpose).mean()\n",
    "\n",
    "print(\"Average Loss of A: \",res_a_sum/len(A_array), \"Average Loss of C\",res_c_sum/len(C_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "# os.chdir('/Users/bluethou/Thesis2020/thesis_pooreumoe_kim/code')\n",
    "os.getcwd()\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_object(Aopt_array, 'Aopt_diff_week.pkl')\n",
    "save_object(Copt_array, 'Copt_diff_week.pkl')\n",
    "save_object(B, 'Bstar_diff_week.pkl')\n",
    "# pickle.dump(tensor, file = open(\"travel_tensor.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(travel_data, 'travel_diff_week.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006894491806455055 0.0006870834315116902\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-64c95f778175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mres_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCt_B\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mXt_mode1_transpose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mres_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB_At\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mXt_mode3_transpose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# optimize B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New Algorithm - meaning less\n",
    "rank =5\n",
    "\n",
    "A = np.random.random((serial_tensors[0].shape[0], rank)) # N node\n",
    "B = np.random.random((serial_tensors[0].shape[1], rank)) # F feature\n",
    "C = np.random.random((serial_tensors[0].shape[2], rank)) # T time\n",
    "\n",
    "for iter in range(10):\n",
    "    A_array = []\n",
    "    C_array = []\n",
    "    Ct_B =0\n",
    "    B_At =0\n",
    "    alpha = 0.001\n",
    "    beta = 0.001\n",
    "    T_interval = len(serial_tensors)\n",
    "\n",
    "#     res_a_sum = 0\n",
    "#     res_c_sum = 0\n",
    "\n",
    "    for t in range(T_interval):\n",
    "      # optimiza A\n",
    "        Ct_B = khatri_rao([C, B])\n",
    "        Xt_mode1_transpose = tl.unfold(serial_tensors[t], mode=0).T\n",
    "        X = np.concatenate((Xt_mode1_transpose, np.sqrt(alpha)*A.T), axis=0)\n",
    "        I = np.identity(A.shape[1])\n",
    "        F = np.concatenate((Ct_B, np.sqrt(alpha)*I))\n",
    "        A = np.linalg.lstsq(F,X, rcond=None)[0].T\n",
    "        A_array.append(A)\n",
    "\n",
    "        # optimize C\n",
    "        B_At = khatri_rao([B, A])\n",
    "        Xt_mode3_transpose = tl.unfold(serial_tensors[t], mode=2).T\n",
    "        X = np.concatenate((Xt_mode3_transpose, np.sqrt(beta)*C.T), axis=0)\n",
    "        I = np.identity(C.shape[1])\n",
    "        F = np.concatenate((B_At, np.sqrt(beta)*I))\n",
    "        C = np.linalg.lstsq(F,X, rcond=None)[0].T\n",
    "        C_array.append(C)\n",
    "\n",
    "        res_a = np.square(Ct_B.dot(A.T) - Xt_mode1_transpose)\n",
    "        res_c = np.square(B_At.dot(C.T) - Xt_mode3_transpose)\n",
    "    \n",
    "    # optimize B\n",
    "    X_concat = tl.unfold(serial_tensors[0], mode=1).T\n",
    "    for x in range(len(serial_tensors)):\n",
    "        if x>0:\n",
    "            XtT_unfold = tl.unfold(serial_tensors[x], mode=1).T\n",
    "            X_concat = np.concatenate((X_concat, XtT_unfold))\n",
    "\n",
    "    F_concat = khatri_rao([C_array[0], A_array[0]])\n",
    "    for i in range(len(C_array)):\n",
    "        if i >0:\n",
    "            F_concat = np.concatenate((F_concat, khatri_rao([C_array[i], A_array[i]])))\n",
    "\n",
    "    B = np.linalg.lstsq(F_concat,X_concat, rcond=None)[0].T\n",
    "\n",
    "    # Adjusted error measure\n",
    "    res_a_sum = 0\n",
    "    res_c_sum = 0\n",
    "    for i in range(len(A_array)):\n",
    "        Ai = A_array[i]\n",
    "        Ci = C_array[i]\n",
    "        res_a_sum += np.square(khatri_rao([Ci, B]).dot(Ai.T) - Xt_mode1_transpose).mean()\n",
    "        res_c_sum += np.square(khatri_rao([B, Ai]).dot(Ci.T) - Xt_mode3_transpose).mean()\n",
    "    print(res_a_sum/len(A_array), res_c_sum/len(C_array))\n",
    "    \n",
    "    # New initialization for the next loop\n",
    "    A = A_array[0] \n",
    "    C = C_array[0]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
