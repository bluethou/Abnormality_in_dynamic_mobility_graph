{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "#datasets\n",
    "import bitcoin_dl as bc\n",
    "import elliptic_temporal_dl as ell_temp\n",
    "import uc_irv_mess_dl as ucim\n",
    "import auto_syst_dl as aus\n",
    "import sbm_dl as sbm\n",
    "import reddit_dl as rdt\n",
    "\n",
    "\n",
    "#taskers\n",
    "import link_pred_tasker as lpt\n",
    "import edge_cls_tasker as ect\n",
    "import node_cls_tasker as nct\n",
    "\n",
    "#models\n",
    "import models as mls\n",
    "import egcn_h\n",
    "import egcn_o\n",
    "\n",
    "\n",
    "import splitter as sp\n",
    "import Cross_Entropy as ce\n",
    "\n",
    "import trainer as tr\n",
    "\n",
    "import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_param_value(param, param_min, param_max, type='int'):\n",
    "\tif str(param) is None or str(param).lower()=='none':\n",
    "\t\tif type=='int':\n",
    "\t\t\treturn random.randrange(param_min, param_max+1)\n",
    "\t\telif type=='logscale':\n",
    "\t\t\tinterval=np.logspace(np.log10(param_min), np.log10(param_max), num=100)\n",
    "\t\t\treturn np.random.choice(interval,1)[0]\n",
    "\t\telse:\n",
    "\t\t\treturn random.uniform(param_min, param_max)\n",
    "\telse:\n",
    "\t\treturn param\n",
    "\n",
    "def build_random_hyper_params(args):\n",
    "\tif args.model == 'all':\n",
    "\t\tmodel_types = ['gcn', 'egcn_o', 'egcn_h', 'gruA', 'gruB','egcn','lstmA', 'lstmB']\n",
    "\t\targs.model=model_types[args.rank]\n",
    "\telif args.model == 'all_nogcn':\n",
    "\t\tmodel_types = ['egcn_o', 'egcn_h', 'gruA', 'gruB','egcn','lstmA', 'lstmB']\n",
    "\t\targs.model=model_types[args.rank]\n",
    "\telif args.model == 'all_noegcn3':\n",
    "\t\tmodel_types = ['gcn', 'egcn_h', 'gruA', 'gruB','egcn','lstmA', 'lstmB']\n",
    "\t\targs.model=model_types[args.rank]\n",
    "\telif args.model == 'all_nogruA':\n",
    "\t\tmodel_types = ['gcn', 'egcn_o', 'egcn_h', 'gruB','egcn','lstmA', 'lstmB']\n",
    "\t\targs.model=model_types[args.rank]\n",
    "\t\targs.model=model_types[args.rank]\n",
    "\telif args.model == 'saveembs':\n",
    "\t\tmodel_types = ['gcn', 'gcn', 'skipgcn', 'skipgcn']\n",
    "\t\targs.model=model_types[args.rank]\n",
    "\n",
    "\targs.learning_rate =random_param_value(args.learning_rate, args.learning_rate_min, args.learning_rate_max, type='logscale')\n",
    "\t# args.adj_mat_time_window = random_param_value(args.adj_mat_time_window, args.adj_mat_time_window_min, args.adj_mat_time_window_max, type='int')\n",
    "\n",
    "\tif args.model == 'gcn':\n",
    "\t\targs.num_hist_steps = 0\n",
    "\telse:\n",
    "\t\targs.num_hist_steps = random_param_value(args.num_hist_steps, args.num_hist_steps_min, args.num_hist_steps_max, type='int')\n",
    "\n",
    "\targs.gcn_parameters['feats_per_node'] =random_param_value(args.gcn_parameters['feats_per_node'], args.gcn_parameters['feats_per_node_min'], args.gcn_parameters['feats_per_node_max'], type='int')\n",
    "\targs.gcn_parameters['layer_1_feats'] =random_param_value(args.gcn_parameters['layer_1_feats'], args.gcn_parameters['layer_1_feats_min'], args.gcn_parameters['layer_1_feats_max'], type='int')\n",
    "\tif args.gcn_parameters['layer_2_feats_same_as_l1'] or args.gcn_parameters['layer_2_feats_same_as_l1'].lower()=='true':\n",
    "\t\targs.gcn_parameters['layer_2_feats'] = args.gcn_parameters['layer_1_feats']\n",
    "\telse:\n",
    "\t\targs.gcn_parameters['layer_2_feats'] =random_param_value(args.gcn_parameters['layer_2_feats'], args.gcn_parameters['layer_1_feats_min'], args.gcn_parameters['layer_1_feats_max'], type='int')\n",
    "\targs.gcn_parameters['lstm_l1_feats'] =random_param_value(args.gcn_parameters['lstm_l1_feats'], args.gcn_parameters['lstm_l1_feats_min'], args.gcn_parameters['lstm_l1_feats_max'], type='int')\n",
    "\tif args.gcn_parameters['lstm_l2_feats_same_as_l1'] or args.gcn_parameters['lstm_l2_feats_same_as_l1'].lower()=='true':\n",
    "\t\targs.gcn_parameters['lstm_l2_feats'] = args.gcn_parameters['lstm_l1_feats']\n",
    "\telse:\n",
    "\t\targs.gcn_parameters['lstm_l2_feats'] =random_param_value(args.gcn_parameters['lstm_l2_feats'], args.gcn_parameters['lstm_l1_feats_min'], args.gcn_parameters['lstm_l1_feats_max'], type='int')\n",
    "\targs.gcn_parameters['cls_feats']=random_param_value(args.gcn_parameters['cls_feats'], args.gcn_parameters['cls_feats_min'], args.gcn_parameters['cls_feats_max'], type='int')\n",
    "\treturn args\n",
    "\n",
    "def build_dataset(args):\n",
    "\tif args.data == 'bitcoinotc' or args.data == 'bitcoinalpha':\n",
    "\t\tif args.data == 'bitcoinotc':\n",
    "\t\t\targs.bitcoin_args = args.bitcoinotc_args\n",
    "\t\telif args.data == 'bitcoinalpha':\n",
    "\t\t\targs.bitcoin_args = args.bitcoinalpha_args\n",
    "\t\treturn bc.bitcoin_dataset(args)\n",
    "\telif args.data == 'aml_sim':\n",
    "\t\treturn aml.Aml_Dataset(args)\n",
    "\telif args.data == 'elliptic':\n",
    "\t\treturn ell.Elliptic_Dataset(args)\n",
    "\telif args.data == 'elliptic_temporal':\n",
    "\t\treturn ell_temp.Elliptic_Temporal_Dataset(args)\n",
    "\telif args.data == 'uc_irv_mess':\n",
    "\t\treturn ucim.Uc_Irvine_Message_Dataset(args)\n",
    "\telif args.data == 'dbg':\n",
    "\t\treturn dbg.dbg_dataset(args)\n",
    "\telif args.data == 'colored_graph':\n",
    "\t\treturn cg.Colored_Graph(args)\n",
    "\telif args.data == 'autonomous_syst':\n",
    "\t\treturn aus.Autonomous_Systems_Dataset(args)\n",
    "\telif args.data == 'reddit':\n",
    "\t\treturn rdt.Reddit_Dataset(args)\n",
    "\telif args.data.startswith('sbm'):\n",
    "\t\tif args.data == 'sbm20':\n",
    "\t\t\targs.sbm_args = args.sbm20_args\n",
    "\t\telif args.data == 'sbm50':\n",
    "\t\t\targs.sbm_args = args.sbm50_args\n",
    "\t\treturn sbm.sbm_dataset(args)\n",
    "\telse:\n",
    "\t\traise NotImplementedError('only arxiv has been implemented')\n",
    "\n",
    "def build_tasker(args,dataset):\n",
    "\tif args.task == 'link_pred':\n",
    "\t\treturn lpt.Link_Pred_Tasker(args,dataset)\n",
    "\telif args.task == 'edge_cls':\n",
    "\t\treturn ect.Edge_Cls_Tasker(args,dataset)\n",
    "\telif args.task == 'node_cls':\n",
    "\t\treturn nct.Node_Cls_Tasker(args,dataset)\n",
    "\telif args.task == 'static_node_cls':\n",
    "\t\treturn nct.Static_Node_Cls_Tasker(args,dataset)\n",
    "\n",
    "\telse:\n",
    "\t\traise NotImplementedError('still need to implement the other tasks')\n",
    "\n",
    "def build_gcn(args,tasker):\n",
    "\tgcn_args = u.Namespace(args.gcn_parameters)\n",
    "\tgcn_args.feats_per_node = tasker.feats_per_node\n",
    "\tif args.model == 'gcn':\n",
    "\t\treturn mls.Sp_GCN(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "\telif args.model == 'skipgcn':\n",
    "\t\treturn mls.Sp_Skip_GCN(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "\telif args.model == 'skipfeatsgcn':\n",
    "\t\treturn mls.Sp_Skip_NodeFeats_GCN(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "\telse:\n",
    "\t\tassert args.num_hist_steps > 0, 'more than one step is necessary to train LSTM'\n",
    "\t\tif args.model == 'lstmA':\n",
    "\t\t\treturn mls.Sp_GCN_LSTM_A(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "\t\telif args.model == 'gruA':\n",
    "\t\t\treturn mls.Sp_GCN_GRU_A(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "\t\telif args.model == 'lstmB':\n",
    "\t\t\treturn mls.Sp_GCN_LSTM_B(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "\t\telif args.model == 'gruB':\n",
    "\t\t\treturn mls.Sp_GCN_GRU_B(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "\t\telif args.model == 'egcn':\n",
    "\t\t\treturn egcn.EGCN(gcn_args, activation = torch.nn.RReLU()).to(args.device)\n",
    "\t\telif args.model == 'egcn_h':\n",
    "\t\t\treturn egcn_h.EGCN(gcn_args, activation = torch.nn.RReLU(), device = args.device)\n",
    "\t\telif args.model == 'skipfeatsegcn_h':\n",
    "\t\t\treturn egcn_h.EGCN(gcn_args, activation = torch.nn.RReLU(), device = args.device, skipfeats=True)\n",
    "\t\telif args.model == 'egcn_o':\n",
    "\t\t\treturn egcn_o.EGCN(gcn_args, activation = torch.nn.RReLU(), device = args.device)\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError('need to finish modifying the models')\n",
    "\n",
    "def build_classifier(args,tasker):\n",
    "\tif 'node_cls' == args.task or 'static_node_cls' == args.task:\n",
    "\t\tmult = 1\n",
    "\telse:\n",
    "\t\tmult = 2\n",
    "\tif 'gru' in args.model or 'lstm' in args.model:\n",
    "\t\tin_feats = args.gcn_parameters['lstm_l2_feats'] * mult\n",
    "\telif args.model == 'skipfeatsgcn' or args.model == 'skipfeatsegcn_h':\n",
    "\t\tin_feats = (args.gcn_parameters['layer_2_feats'] + args.gcn_parameters['feats_per_node']) * mult\n",
    "\telse:\n",
    "\t\tin_feats = args.gcn_parameters['layer_2_feats'] * mult\n",
    "\n",
    "\treturn mls.Classifier(args,in_features = in_feats, out_features = tasker.num_classes).to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config_file CONFIG_FILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/gimpuleumoe/Library/Jupyter/runtime/kernel-64e77cd2-61b1-4d06-a6af-0a3dece9081e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = u.create_parser()\n",
    "args = u.parse_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config_file CONFIG_FILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/gimpuleumoe/Library/Jupyter/runtime/kernel-64e77cd2-61b1-4d06-a6af-0a3dece9081e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = u.create_parser()\n",
    "args = u.parse_args(parser)\n",
    "\n",
    "global rank, wsize, use_cuda\n",
    "args.use_cuda = (torch.cuda.is_available() and args.use_cuda)\n",
    "args.device='cpu'\n",
    "if args.use_cuda:\n",
    "    args.device='cuda'\n",
    "print (\"use CUDA:\", args.use_cuda, \"- device:\", args.device)\n",
    "try:\n",
    "    dist.init_process_group(backend='mpi') #, world_size=4\n",
    "    rank = dist.get_rank()\n",
    "    wsize = dist.get_world_size()\n",
    "    print('Hello from process {} (out of {})'.format(dist.get_rank(), dist.get_world_size()))\n",
    "    if args.use_cuda:\n",
    "        torch.cuda.set_device(rank )  # are we sure of the rank+1????\n",
    "        print('using the device {}'.format(torch.cuda.current_device()))\n",
    "except:\n",
    "    rank = 0\n",
    "    wsize = 1\n",
    "    print(('MPI backend not preset. Set process rank to {} (out of {})'.format(rank,\n",
    "                                                                               wsize)))\n",
    "\n",
    "if args.seed is None and args.seed!='None':\n",
    "    seed = 123+rank#int(time.time())+rank\n",
    "else:\n",
    "    seed=args.seed#+rank\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "args.seed=seed\n",
    "args.rank=rank\n",
    "args.wsize=wsize\n",
    "\n",
    "# Assign the requested random hyper parameters\n",
    "args = build_random_hyper_params(args)\n",
    "\n",
    "#build the dataset\n",
    "dataset = build_dataset(args)\n",
    "#build the tasker\n",
    "tasker = build_tasker(args,dataset)\n",
    "#build the splitter\n",
    "splitter = sp.splitter(args,tasker)\n",
    "#build the models\n",
    "gcn = build_gcn(args, tasker)\n",
    "classifier = build_classifier(args,tasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
